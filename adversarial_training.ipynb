{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whsg1XX_OZs6"
      },
      "source": [
        "# Boilerplate\n",
        "\n",
        "Packae installation, loading, and dataloaders. There's also a simple model defined. You can change it your favourite architecture if you want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R1domTvnONqD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow (d:\\conda_jinyangworkenv\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow (d:\\conda_jinyangworkenv\\lib\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in d:\\conda_jinyangworkenv\\lib\\site-packages (2.6.2.2)\n",
            "Requirement already satisfied: numpy in d:\\conda_jinyangworkenv\\lib\\site-packages (from tensorboardX) (1.22.4)\n",
            "Requirement already satisfied: packaging in d:\\conda_jinyangworkenv\\lib\\site-packages (from tensorboardX) (23.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in d:\\conda_jinyangworkenv\\lib\\site-packages (from tensorboardX) (4.22.1)\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "9913344it [00:00, 13309719.94it/s]                             \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_data/MNIST\\raw\\train-images-idx3-ubyte.gz to mnist_data/MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "29696it [00:00, ?it/s]                   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_data/MNIST\\raw\\train-labels-idx1-ubyte.gz to mnist_data/MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1649664it [00:00, 16744491.61it/s]         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_data/MNIST\\raw\\t10k-images-idx3-ubyte.gz to mnist_data/MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5120it [00:00, ?it/s]                   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting mnist_data/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to mnist_data/MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Normalize()\n",
              "  (1): Net(\n",
              "    (fc): Linear(in_features=784, out_features=200, bias=True)\n",
              "    (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install tensorboardX\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "batch_size = 64\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "## Dataloaders\n",
        "train_dataset = datasets.MNIST('mnist_data/', train=True, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "test_dataset = datasets.MNIST('mnist_data/', train=False, download=True, transform=transforms.Compose(\n",
        "    [transforms.ToTensor()]\n",
        "))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "## Simple NN. You can change this if you want.\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc = nn.Linear(28*28, 200)\n",
        "        self.fc2 = nn.Linear(200,10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view((-1, 28*28))\n",
        "        x = F.relu(self.fc(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class Normalize(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return (x - 0.1307)/0.3081\n",
        "\n",
        "# Add the data normalization as a first \"layer\" to the network\n",
        "# this allows us to search for adverserial examples to the real image, rather than\n",
        "# to the normalized image\n",
        "model = nn.Sequential(Normalize(), Net())\n",
        "\n",
        "model = model.to(device)\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCmWfZHTO8Oo"
      },
      "source": [
        "# Implement the Attacks\n",
        "\n",
        "Functions are given a simple useful signature that you can start with. Feel free to extend the signature as you see fit.\n",
        "\n",
        "You may find it useful to create a 'batched' version of PGD that you can use to create the adversarial attack."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EZjvA49yONqP"
      },
      "outputs": [],
      "source": [
        "# The last argument 'targeted' can be used to toggle between a targeted and untargeted attack.\n",
        "def fgsm(model, x, target, eps, targeted=True):\n",
        "    ###############################################\n",
        "    # TODO fill me\n",
        "    adv_x = []\n",
        "    if targeted:\n",
        "        x.requires_grad_() # this is required so we can compute the gradient w.r.t x\n",
        "        # compute gradient\n",
        "        # note that CrossEntropyLoss() combines the cross-entropy loss and an implicit softmax function\n",
        "        L = nn.CrossEntropyLoss()\n",
        "        loss = L(model(x), target) # TO LEARN: make sure you understand this line\n",
        "        loss.backward()\n",
        "        adv_x = x - eps * torch.sign(x.grad)\n",
        "        new_class = model(adv_x).argmax(dim=1).item()\n",
        "        assert(new_class == target)\n",
        "    else:\n",
        "        x.requires_grad_() # this is required so we can compute the gradient w.r.t x\n",
        "        # compute gradient\n",
        "        # note that CrossEntropyLoss() combines the cross-entropy loss and an implicit softmax function\n",
        "        L = nn.CrossEntropyLoss()\n",
        "        loss = L(model(x), target) # TO LEARN: make sure you understand this line\n",
        "        loss.backward()\n",
        "        adv_x = x + eps * torch.sign(x.grad)\n",
        "    ###############################################\n",
        "    return adv_x\n",
        "\n",
        "\n",
        "def pgd_untargeted(model, x, labels, k, eps, eps_step):\n",
        "    ###############################################\n",
        "    # TODO fill me\n",
        "    # print(x)\n",
        "    alpha = eps_step\n",
        "    adv_x = x.clone().detach().requires_grad_(True).to(device)\n",
        "    for i in range(k):\n",
        "        L = nn.CrossEntropyLoss()\n",
        "        loss = L(model(adv_x), labels) # TO LEARN: make sure you understand this line\n",
        "        loss.backward()\n",
        "        # print(adv_x)\n",
        "        pertu = alpha * torch.sign(adv_x.grad)\n",
        "        adv_x = torch.clip(adv_x + pertu, min=x-eps, max=x+eps)\n",
        "        adv_x = adv_x.detach().clone()\n",
        "        adv_x.requires_grad_() \n",
        "    ###############################################\n",
        "    return adv_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Mja_AB4RykO"
      },
      "source": [
        "# Implement Adversarial Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "V-sw8yKYONqQ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, num_epochs, enable_defense=True):\n",
        "    learning_rate = 0.0001\n",
        "\n",
        "    opt = optim.Adam(params=model.parameters(), lr=learning_rate)\n",
        "\n",
        "    ce_loss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    tot_steps = 0\n",
        "\n",
        "    for epoch in range(1,num_epochs+1):\n",
        "        t1 = time.time()\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(train_loader):\n",
        "\n",
        "            if enable_defense:\n",
        "                ###############################################\n",
        "                # Fill code here to do adversarial training\n",
        "                # You may find it useful to switch to 'eval' model while generating the attack\n",
        "                # and switch back again to 'train' mode once the attack is generated.\n",
        "                model.eval()\n",
        "                x_batch = pgd_untargeted(model, x_batch, y_batch, k=5, eps=0.4, eps_step=0.01)\n",
        "                model.train()\n",
        "                ###############################################\n",
        "\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            tot_steps += 1\n",
        "            opt.zero_grad()\n",
        "            out = model(x_batch)\n",
        "            batch_loss = ce_loss(out, y_batch)\n",
        "            batch_loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        tot_test, tot_acc = 0.0, 0.0\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(test_loader):\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "            out = model(x_batch)\n",
        "            pred = torch.max(out, dim=1)[1]\n",
        "            acc = pred.eq(y_batch).sum().item()\n",
        "            tot_acc += acc\n",
        "            tot_test += x_batch.size()[0]\n",
        "        t2 = time.time()\n",
        "\n",
        "        print('Epoch %d: Accuracy %.5lf [%.2lf seconds]' % (epoch, tot_acc/tot_test, t2-t1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPMdfEhtR3zm"
      },
      "source": [
        "# Study Accuracy, Quality, etc.\n",
        "\n",
        "Compare the various results and report your observations on the submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ufD-ccTFR8R2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Accuracy 0.97570 [20.33 seconds]\n",
            "Epoch 2: Accuracy 0.97780 [20.38 seconds]\n",
            "Epoch 3: Accuracy 0.97910 [19.81 seconds]\n",
            "Epoch 4: Accuracy 0.97830 [22.88 seconds]\n",
            "Epoch 5: Accuracy 0.98000 [19.80 seconds]\n",
            "Epoch 6: Accuracy 0.98060 [23.16 seconds]\n",
            "Epoch 7: Accuracy 0.98070 [23.29 seconds]\n",
            "Epoch 8: Accuracy 0.98120 [20.25 seconds]\n",
            "Epoch 9: Accuracy 0.98130 [22.15 seconds]\n",
            "Epoch 10: Accuracy 0.98180 [21.54 seconds]\n",
            "Epoch 11: Accuracy 0.98200 [21.25 seconds]\n",
            "Epoch 12: Accuracy 0.98260 [21.57 seconds]\n",
            "Epoch 13: Accuracy 0.98240 [22.64 seconds]\n",
            "Epoch 14: Accuracy 0.98290 [21.20 seconds]\n",
            "Epoch 15: Accuracy 0.98260 [20.36 seconds]\n",
            "Epoch 16: Accuracy 0.98310 [23.86 seconds]\n",
            "Epoch 17: Accuracy 0.98340 [22.25 seconds]\n",
            "Epoch 18: Accuracy 0.98310 [22.06 seconds]\n",
            "Epoch 19: Accuracy 0.98420 [22.05 seconds]\n",
            "Epoch 20: Accuracy 0.98340 [20.30 seconds]\n"
          ]
        }
      ],
      "source": [
        "train_model(model, 20, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Accuracy 0.97190 [30.25 seconds]\n",
            "Epoch 2: Accuracy 0.97230 [31.57 seconds]\n",
            "Epoch 3: Accuracy 0.97090 [28.96 seconds]\n",
            "Epoch 4: Accuracy 0.97270 [29.20 seconds]\n",
            "Epoch 5: Accuracy 0.97180 [29.49 seconds]\n",
            "Epoch 6: Accuracy 0.97200 [28.95 seconds]\n",
            "Epoch 7: Accuracy 0.97120 [29.31 seconds]\n",
            "Epoch 8: Accuracy 0.97220 [33.76 seconds]\n",
            "Epoch 9: Accuracy 0.97220 [32.49 seconds]\n",
            "Epoch 10: Accuracy 0.97220 [36.72 seconds]\n"
          ]
        }
      ],
      "source": [
        "train_model(model, 10, True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
